# Papers

- When studying Transformers, LSTMs, LLMs: [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)
- When modelling volatility: [Variational Autoencoders: A Hands-Off Approach to Volatility](https://arxiv.org/pdf/2102.03945.pdf)
- When covering Deep Neural Networks gradient instability (notably with the study of Vanishing/Exploding Gradients Problem): [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- When covering Nonsaturating Activation Functions (notably with leacky ReLu): [Empirical Evaluation of Rectified Activations in Convolution Network](https://arxiv.org/pdf/1505.00853.pdf)
- When covering activation functions (notably The exponential linear unit(ELU)) and their challenges: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)
- When covering activation functions (notably  the Scaled exponential linear unit (SELU) activation function): [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf)
- When covering batch normalisation: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)
- When covering batch normalization and its substitutes such as the fixed_update weights presented in the following paper: [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/pdf/1901.09321.pdf)
- When covering Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
- When covering GloVe: [All-but-the-top: Simple And Effective Processing For Word Representations](https://arxiv.org/pdf/1702.01417.pdf)
- When creating the NN architecture of credit scoring, controlling the flow of info between categorical and non-categorical label-encoded data using GRNs [Language Modeling with Gated Convolutional Networks](https://arxiv.org/pdf/1612.08083.pdf)
- When working on attention mechanisms. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v5.pdf)
- [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
