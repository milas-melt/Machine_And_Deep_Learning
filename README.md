# Papers
- When studying Transformers, LSTMs, LLMs: [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)
- When modelling volatility: [Variational Autoencoders: A Hands-Off Approach to Volatility](https://arxiv.org/pdf/2102.03945.pdf)
- When covering Deep Neural Networks gradient instability (notably with the study of Vanishing/Exploding Gradients Problem): [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- When covering Nonsaturating Activation Functions (notably with leacky ReLu): [Empirical Evaluation of Rectified Activations in Convolution Network](https://arxiv.org/pdf/1505.00853.pdf)
- When covering activation functions (notably The exponential linear unit(ELU)) and their challenges: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)
- When covering activation functions (notably  the Scaled exponential linear unit (SELU) activation function): [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf)
- When covering batch normalisation: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)
- When covering batch normalization and its substitutes such as the fixed_update weights presented in the following paper: [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/pdf/1901.09321.pdf)
- When covering Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
- When covering GloVe: [All-but-the-top: Simple And Effective Processing For Word Representations](https://arxiv.org/pdf/1702.01417.pdf)
